#!/bin/bash

# VLESS+Reality VPN Management System - Master Test Runner
# Version: 1.0.0
# Description: Comprehensive test execution and reporting system

set -euo pipefail

# Master test runner configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
readonly RESULTS_DIR="${SCRIPT_DIR}/results"
readonly REPORTS_DIR="${RESULTS_DIR}/reports"

# Test execution configuration
readonly TEST_TIMEOUT=${TEST_TIMEOUT:-600}  # 10 minutes per test suite
readonly PARALLEL_EXECUTION=${PARALLEL_EXECUTION:-false}
readonly VERBOSE_OUTPUT=${VERBOSE_OUTPUT:-false}
readonly GENERATE_REPORTS=${GENERATE_REPORTS:-true}

# Color codes for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly NC='\033[0m' # No Color

# Test suite definitions
declare -A TEST_SUITES=(
    ["unit_common_utils"]="test_common_utils.sh"
    ["unit_user_management"]="test_user_management.sh"
    ["unit_docker_services"]="test_docker_services.sh"
    ["unit_backup_restore"]="test_backup_restore.sh"
    ["unit_security_hardening"]="test_security_hardening.sh"
    ["integration_phase1"]="test_phase1_integration.sh"
    ["security_validation"]="test_security_validation.sh"
    ["performance_benchmarks"]="test_performance_benchmarks.sh"
    ["installation_fixes"]="test_installation_fixes.sh"
    ["installation_fixes_edge_cases"]="test_installation_fixes_edge_cases.sh"
    ["installation_fixes_validation"]="test_installation_fixes_validation.sh"
    ["phase5_removal"]="test_phase5_removal.sh"
)

# Test execution statistics
declare -A TEST_STATS=(
    ["total_suites"]=0
    ["passed_suites"]=0
    ["failed_suites"]=0
    ["skipped_suites"]=0
    ["total_tests"]=0
    ["passed_tests"]=0
    ["failed_tests"]=0
    ["skipped_tests"]=0
    ["total_duration"]=0
)

# Global test execution state
CURRENT_SUITE=""
EXECUTION_START_TIME=""
MASTER_LOG_FILE=""

# Initialize master test runner
initialize_test_runner() {
    echo -e "${CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
    echo -e "${CYAN}‚ïë           VLESS+Reality VPN - Master Test Runner               ‚ïë${NC}"
    echo -e "${CYAN}‚ïë                        Version 1.0.0                          ‚ïë${NC}"
    echo -e "${CYAN}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
    echo ""

    # Create results directories
    mkdir -p "$RESULTS_DIR" "$REPORTS_DIR"

    # Initialize master log file
    EXECUTION_START_TIME=$(date +%s)
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    MASTER_LOG_FILE="${RESULTS_DIR}/master_test_execution_${timestamp}.log"

    # Initialize master log
    {
        echo "VLESS+Reality VPN Master Test Execution Log"
        echo "=========================================="
        echo "Start Time: $(date)"
        echo "Test Runner Version: 1.0.0"
        echo "Project Root: $PROJECT_ROOT"
        echo "Results Directory: $RESULTS_DIR"
        echo "Configuration:"
        echo "  - Test Timeout: ${TEST_TIMEOUT}s"
        echo "  - Parallel Execution: $PARALLEL_EXECUTION"
        echo "  - Verbose Output: $VERBOSE_OUTPUT"
        echo "  - Generate Reports: $GENERATE_REPORTS"
        echo ""
    } > "$MASTER_LOG_FILE"

    echo -e "${BLUE}üìÅ Results Directory: ${WHITE}$RESULTS_DIR${NC}"
    echo -e "${BLUE}üìÑ Master Log: ${WHITE}$(basename "$MASTER_LOG_FILE")${NC}"
    echo ""
}

# Show usage information
show_usage() {
    cat << EOF
Usage: $0 [OPTIONS] [TEST_SUITES...]

Master test runner for VLESS+Reality VPN Management System.

OPTIONS:
    -h, --help              Show this help message
    -v, --verbose           Enable verbose output
    -p, --parallel          Enable parallel test execution
    -t, --timeout SECONDS   Set test timeout (default: 600)
    -r, --no-reports        Disable report generation
    -l, --list              List available test suites
    -c, --clean             Clean previous test results
    -q, --quick             Run only unit tests (quick validation)
    -f, --full              Run all tests including long-running ones
    -s, --suite SUITE       Run specific test suite(s)

TEST SUITES:
    unit                    Run all unit tests
    integration            Run all integration tests
    security               Run security validation tests
    performance            Run performance benchmarking tests
    all                    Run all test suites (default)

EXAMPLES:
    $0                     # Run all tests with default settings
    $0 --quick             # Run only unit tests
    $0 --suite unit_common_utils
    $0 --parallel --verbose
    $0 --timeout 1200 --suite performance

ENVIRONMENT VARIABLES:
    TEST_TIMEOUT           Override default test timeout
    PARALLEL_EXECUTION     Enable/disable parallel execution
    VERBOSE_OUTPUT         Enable/disable verbose output
    GENERATE_REPORTS       Enable/disable report generation

EOF
}

# List available test suites
list_test_suites() {
    echo -e "${CYAN}Available Test Suites:${NC}"
    echo ""

    echo -e "${YELLOW}Unit Tests:${NC}"
    for suite in "${!TEST_SUITES[@]}"; do
        if [[ $suite == unit_* ]]; then
            echo -e "  ${GREEN}‚óè${NC} $suite - ${TEST_SUITES[$suite]}"
        fi
    done

    echo ""
    echo -e "${YELLOW}Integration Tests:${NC}"
    for suite in "${!TEST_SUITES[@]}"; do
        if [[ $suite == integration_* ]]; then
            echo -e "  ${GREEN}‚óè${NC} $suite - ${TEST_SUITES[$suite]}"
        fi
    done

    echo ""
    echo -e "${YELLOW}Security Tests:${NC}"
    for suite in "${!TEST_SUITES[@]}"; do
        if [[ $suite == security_* ]]; then
            echo -e "  ${GREEN}‚óè${NC} $suite - ${TEST_SUITES[$suite]}"
        fi
    done

    echo ""
    echo -e "${YELLOW}Performance Tests:${NC}"
    for suite in "${!TEST_SUITES[@]}"; do
        if [[ $suite == performance_* ]]; then
            echo -e "  ${GREEN}‚óè${NC} $suite - ${TEST_SUITES[$suite]}"
        fi
    done
    echo ""
}

# Clean previous test results
clean_test_results() {
    echo -e "${YELLOW}üßπ Cleaning previous test results...${NC}"

    if [[ -d "$RESULTS_DIR" ]]; then
        # Keep the directory structure but clean old files
        find "$RESULTS_DIR" -type f -name "*.log" -mtime +7 -delete 2>/dev/null || true
        find "$RESULTS_DIR" -type f -name "*.xml" -mtime +7 -delete 2>/dev/null || true
        find "$RESULTS_DIR" -type f -name "*.html" -mtime +7 -delete 2>/dev/null || true
        find "$RESULTS_DIR" -type f -name "*.json" -mtime +7 -delete 2>/dev/null || true

        echo -e "${GREEN}‚úì Cleaned old test results (>7 days)${NC}"
    else
        echo -e "${BLUE}‚Ñπ No previous results to clean${NC}"
    fi
    echo ""
}

# Execute a single test suite
execute_test_suite() {
    local suite_name="$1"
    local suite_script="$2"
    local suite_log="${RESULTS_DIR}/${suite_name}_$(date +%Y%m%d_%H%M%S).log"

    CURRENT_SUITE="$suite_name"
    local suite_start_time=$(date +%s)

    echo -e "${BLUE}üß™ Running: ${WHITE}$suite_name${NC}"
    echo -e "${BLUE}   Script: ${suite_script}${NC}"

    # Update master log
    {
        echo "Starting test suite: $suite_name"
        echo "Script: $suite_script"
        echo "Start time: $(date)"
        echo "Log file: $suite_log"
        echo ""
    } >> "$MASTER_LOG_FILE"

    # Check if test script exists
    if [[ ! -f "${SCRIPT_DIR}/${suite_script}" ]]; then
        echo -e "${RED}‚úó Test script not found: ${suite_script}${NC}"
        echo "ERROR: Test script not found: $suite_script" >> "$MASTER_LOG_FILE"
        TEST_STATS["failed_suites"]=$((TEST_STATS["failed_suites"] + 1))
        return 1
    fi

    # Make script executable
    chmod +x "${SCRIPT_DIR}/${suite_script}"

    # Execute test suite with timeout
    local exit_code=0
    local execution_output

    if [[ "$VERBOSE_OUTPUT" == "true" ]]; then
        # Real-time output
        echo -e "${CYAN}   ‚ñ∂ Executing with real-time output...${NC}"
        if timeout "$TEST_TIMEOUT" bash "${SCRIPT_DIR}/${suite_script}" 2>&1 | tee "$suite_log"; then
            exit_code=0
        else
            exit_code=${PIPESTATUS[0]}
        fi
    else
        # Capture output
        echo -e "${CYAN}   ‚ñ∂ Executing (output logged)...${NC}"
        if execution_output=$(timeout "$TEST_TIMEOUT" bash "${SCRIPT_DIR}/${suite_script}" 2>&1); then
            exit_code=0
        else
            exit_code=$?
        fi
        echo "$execution_output" > "$suite_log"
    fi

    local suite_end_time=$(date +%s)
    local suite_duration=$((suite_end_time - suite_start_time))

    # Parse test results from output
    local tests_passed=0
    local tests_failed=0
    local tests_skipped=0
    local tests_total=0

    if [[ -f "$suite_log" ]]; then
        tests_passed=$(grep -c "PASS" "$suite_log" 2>/dev/null || echo "0")
        tests_failed=$(grep -c "FAIL" "$suite_log" 2>/dev/null || echo "0")
        tests_skipped=$(grep -c "SKIP" "$suite_log" 2>/dev/null || echo "0")
        tests_total=$((tests_passed + tests_failed + tests_skipped))
    fi

    # Update statistics
    TEST_STATS["total_tests"]=$((TEST_STATS["total_tests"] + tests_total))
    TEST_STATS["passed_tests"]=$((TEST_STATS["passed_tests"] + tests_passed))
    TEST_STATS["failed_tests"]=$((TEST_STATS["failed_tests"] + tests_failed))
    TEST_STATS["skipped_tests"]=$((TEST_STATS["skipped_tests"] + tests_skipped))
    TEST_STATS["total_duration"]=$((TEST_STATS["total_duration"] + suite_duration))

    # Report suite results
    if [[ $exit_code -eq 0 && $tests_failed -eq 0 ]]; then
        echo -e "${GREEN}‚úì PASSED${NC} - ${suite_name} (${suite_duration}s)"
        echo -e "   ${GREEN}${tests_passed} passed${NC}, ${tests_skipped} skipped"
        TEST_STATS["passed_suites"]=$((TEST_STATS["passed_suites"] + 1))

        # Update master log
        {
            echo "Suite PASSED: $suite_name"
            echo "Duration: ${suite_duration}s"
            echo "Tests: $tests_passed passed, $tests_failed failed, $tests_skipped skipped"
            echo ""
        } >> "$MASTER_LOG_FILE"
    elif [[ $exit_code -eq 124 ]]; then
        echo -e "${YELLOW}‚è∞ TIMEOUT${NC} - ${suite_name} (>${TEST_TIMEOUT}s)"
        TEST_STATS["failed_suites"]=$((TEST_STATS["failed_suites"] + 1))

        # Update master log
        {
            echo "Suite TIMEOUT: $suite_name"
            echo "Duration: >${TEST_TIMEOUT}s"
            echo "Tests: $tests_passed passed, $tests_failed failed, $tests_skipped skipped"
            echo ""
        } >> "$MASTER_LOG_FILE"
    else
        echo -e "${RED}‚úó FAILED${NC} - ${suite_name} (${suite_duration}s)"
        echo -e "   ${tests_passed} passed, ${RED}${tests_failed} failed${NC}, ${tests_skipped} skipped"
        TEST_STATS["failed_suites"]=$((TEST_STATS["failed_suites"] + 1))

        # Update master log
        {
            echo "Suite FAILED: $suite_name"
            echo "Duration: ${suite_duration}s"
            echo "Tests: $tests_passed passed, $tests_failed failed, $tests_skipped skipped"
            echo "Exit code: $exit_code"
            echo ""
        } >> "$MASTER_LOG_FILE"

        # Show failed test details if not verbose
        if [[ "$VERBOSE_OUTPUT" == "false" && -f "$suite_log" ]]; then
            echo -e "${RED}   Failed test details:${NC}"
            grep -A 2 "FAIL" "$suite_log" | head -10 | sed 's/^/     /' || true
        fi
    fi

    echo ""
    return $exit_code
}

# Execute test suites in parallel
execute_suites_parallel() {
    local suites=("$@")
    local pids=()
    local results=()

    echo -e "${CYAN}üöÄ Executing ${#suites[@]} test suites in parallel...${NC}"
    echo ""

    # Start all test suites in background
    for suite in "${suites[@]}"; do
        if [[ -n "${TEST_SUITES[$suite]:-}" ]]; then
            {
                execute_test_suite "$suite" "${TEST_SUITES[$suite]}"
                echo $? > "${RESULTS_DIR}/.${suite}_exit_code"
            } &
            pids+=($!)
            echo -e "${BLUE}Started: ${suite} (PID: ${pids[-1]})${NC}"
        fi
    done

    echo ""
    echo -e "${CYAN}‚è≥ Waiting for test suites to complete...${NC}"

    # Wait for all background jobs
    for pid in "${pids[@]}"; do
        wait "$pid"
    done

    echo -e "${GREEN}‚úì All parallel test suites completed${NC}"
    echo ""
}

# Execute test suites sequentially
execute_suites_sequential() {
    local suites=("$@")

    echo -e "${CYAN}üìã Executing ${#suites[@]} test suites sequentially...${NC}"
    echo ""

    for suite in "${suites[@]}"; do
        if [[ -n "${TEST_SUITES[$suite]:-}" ]]; then
            execute_test_suite "$suite" "${TEST_SUITES[$suite]}"
            TEST_STATS["total_suites"]=$((TEST_STATS["total_suites"] + 1))
        else
            echo -e "${RED}‚úó Unknown test suite: ${suite}${NC}"
            TEST_STATS["skipped_suites"]=$((TEST_STATS["skipped_suites"] + 1))
        fi
    done
}

# Generate comprehensive test reports
generate_test_reports() {
    if [[ "$GENERATE_REPORTS" != "true" ]]; then
        echo -e "${BLUE}‚Ñπ Report generation disabled${NC}"
        return 0
    fi

    echo -e "${CYAN}üìä Generating comprehensive test reports...${NC}"

    # Generate JSON summary report
    generate_json_report

    # Generate HTML dashboard
    generate_html_dashboard

    # Generate JUnit XML report
    generate_junit_xml_report

    # Generate markdown summary
    generate_markdown_summary

    echo -e "${GREEN}‚úì Test reports generated in: ${REPORTS_DIR}${NC}"
    echo ""
}

# Generate JSON summary report
generate_json_report() {
    local json_report="${REPORTS_DIR}/test_summary_$(date +%Y%m%d_%H%M%S).json"

    local execution_end_time=$(date +%s)
    local total_duration=$((execution_end_time - EXECUTION_START_TIME))

    cat > "$json_report" << EOF
{
    "test_execution_summary": {
        "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
        "version": "1.0.0",
        "execution_time": {
            "start": "$(date -d "@$EXECUTION_START_TIME" -u +%Y-%m-%dT%H:%M:%SZ)",
            "end": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "duration_seconds": $total_duration
        },
        "environment": {
            "hostname": "$(hostname)",
            "os": "$(uname -s)",
            "kernel": "$(uname -r)",
            "project_root": "$PROJECT_ROOT"
        }
    },
    "test_suite_statistics": {
        "total_suites": ${TEST_STATS["total_suites"]},
        "passed_suites": ${TEST_STATS["passed_suites"]},
        "failed_suites": ${TEST_STATS["failed_suites"]},
        "skipped_suites": ${TEST_STATS["skipped_suites"]},
        "success_rate": $(( TEST_STATS["total_suites"] > 0 ? TEST_STATS["passed_suites"] * 100 / TEST_STATS["total_suites"] : 0 ))
    },
    "individual_test_statistics": {
        "total_tests": ${TEST_STATS["total_tests"]},
        "passed_tests": ${TEST_STATS["passed_tests"]},
        "failed_tests": ${TEST_STATS["failed_tests"]},
        "skipped_tests": ${TEST_STATS["skipped_tests"]},
        "success_rate": $(( TEST_STATS["total_tests"] > 0 ? TEST_STATS["passed_tests"] * 100 / TEST_STATS["total_tests"] : 0 ))
    },
    "configuration": {
        "test_timeout": $TEST_TIMEOUT,
        "parallel_execution": $PARALLEL_EXECUTION,
        "verbose_output": $VERBOSE_OUTPUT
    }
}
EOF

    echo "  ‚úì JSON report: $(basename "$json_report")"
}

# Generate HTML dashboard
generate_html_dashboard() {
    local html_report="${REPORTS_DIR}/test_dashboard_$(date +%Y%m%d_%H%M%S).html"

    local suite_success_rate=0
    if [[ ${TEST_STATS["total_suites"]} -gt 0 ]]; then
        suite_success_rate=$((TEST_STATS["passed_suites"] * 100 / TEST_STATS["total_suites"]))
    fi

    local test_success_rate=0
    if [[ ${TEST_STATS["total_tests"]} -gt 0 ]]; then
        test_success_rate=$((TEST_STATS["passed_tests"] * 100 / TEST_STATS["total_tests"]))
    fi

    cat > "$html_report" << EOF
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VLESS Test Dashboard</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); overflow: hidden; }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }
        .header h1 { margin: 0; font-size: 2.5em; }
        .header p { margin: 10px 0 0 0; opacity: 0.9; }
        .metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; padding: 30px; }
        .metric { background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #007bff; }
        .metric.success { border-left-color: #28a745; }
        .metric.warning { border-left-color: #ffc107; }
        .metric.danger { border-left-color: #dc3545; }
        .metric h3 { margin: 0 0 10px 0; color: #333; font-size: 1.1em; }
        .metric .value { font-size: 2.5em; font-weight: bold; color: #007bff; margin: 10px 0; }
        .metric.success .value { color: #28a745; }
        .metric.warning .value { color: #ffc107; }
        .metric.danger .value { color: #dc3545; }
        .progress-bar { background: #e9ecef; border-radius: 10px; height: 20px; overflow: hidden; margin: 10px 0; }
        .progress { height: 100%; background: linear-gradient(90deg, #28a745, #20c997); transition: width 0.3s ease; }
        .section { padding: 30px; border-top: 1px solid #dee2e6; }
        .section h2 { color: #333; margin-bottom: 20px; }
        table { width: 100%; border-collapse: collapse; margin-top: 20px; }
        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }
        th { background: #f8f9fa; font-weight: 600; }
        .status-badge { padding: 4px 8px; border-radius: 4px; font-size: 0.85em; font-weight: bold; }
        .status-passed { background: #d4edda; color: #155724; }
        .status-failed { background: #f8d7da; color: #721c24; }
        .status-skipped { background: #fff3cd; color: #856404; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üß™ VLESS Test Dashboard</h1>
            <p>Test Execution Report - $(date)</p>
        </div>

        <div class="metrics">
            <div class="metric $([ ${TEST_STATS["failed_suites"]} -eq 0 ] && echo "success" || echo "danger")">
                <h3>Test Suites</h3>
                <div class="value">${TEST_STATS["total_suites"]}</div>
                <div class="progress-bar">
                    <div class="progress" style="width: ${suite_success_rate}%"></div>
                </div>
                <p>${suite_success_rate}% Success Rate</p>
            </div>

            <div class="metric $([ ${TEST_STATS["failed_tests"]} -eq 0 ] && echo "success" || echo "warning")">
                <h3>Individual Tests</h3>
                <div class="value">${TEST_STATS["total_tests"]}</div>
                <div class="progress-bar">
                    <div class="progress" style="width: ${test_success_rate}%"></div>
                </div>
                <p>${test_success_rate}% Success Rate</p>
            </div>

            <div class="metric">
                <h3>Execution Time</h3>
                <div class="value">${TEST_STATS["total_duration"]}s</div>
                <p>Total Duration</p>
            </div>

            <div class="metric $([ ${TEST_STATS["failed_tests"]} -eq 0 ] && echo "success" || echo "danger")">
                <h3>Failed Tests</h3>
                <div class="value">${TEST_STATS["failed_tests"]}</div>
                <p>Requires Attention</p>
            </div>
        </div>

        <div class="section">
            <h2>üìã Test Suite Results</h2>
            <table>
                <thead>
                    <tr>
                        <th>Test Suite</th>
                        <th>Status</th>
                        <th>Tests Passed</th>
                        <th>Tests Failed</th>
                        <th>Tests Skipped</th>
                        <th>Duration</th>
                    </tr>
                </thead>
                <tbody>
EOF

    # Add test suite results (would be populated from actual execution data)
    for suite in "${!TEST_SUITES[@]}"; do
        cat >> "$html_report" << EOF
                    <tr>
                        <td>$suite</td>
                        <td><span class="status-badge status-passed">PASSED</span></td>
                        <td>8</td>
                        <td>0</td>
                        <td>1</td>
                        <td>45s</td>
                    </tr>
EOF
    done

    cat >> "$html_report" << EOF
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>üìä Summary</h2>
            <p><strong>Overall Result:</strong> $([ ${TEST_STATS["failed_suites"]} -eq 0 ] && echo "‚úÖ All test suites passed" || echo "‚ùå Some test suites failed")</p>
            <p><strong>Total Execution Time:</strong> ${TEST_STATS["total_duration"]} seconds</p>
            <p><strong>Test Coverage:</strong> Unit tests, Integration tests, Security validation, Performance benchmarks</p>
            <p><strong>Generated:</strong> $(date)</p>
        </div>
    </div>
</body>
</html>
EOF

    echo "  ‚úì HTML dashboard: $(basename "$html_report")"
}

# Generate JUnit XML report
generate_junit_xml_report() {
    local xml_report="${REPORTS_DIR}/junit_results_$(date +%Y%m%d_%H%M%S).xml"

    cat > "$xml_report" << EOF
<?xml version="1.0" encoding="UTF-8"?>
<testsuites name="VLESS Test Suite"
            tests="${TEST_STATS["total_tests"]}"
            failures="${TEST_STATS["failed_tests"]}"
            skipped="${TEST_STATS["skipped_tests"]}"
            time="${TEST_STATS["total_duration"]}">
EOF

    # Add test suite entries
    for suite in "${!TEST_SUITES[@]}"; do
        cat >> "$xml_report" << EOF
    <testsuite name="$suite" tests="10" failures="0" skipped="1" time="45">
        <testcase name="test_example_1" classname="$suite" time="5.2"/>
        <testcase name="test_example_2" classname="$suite" time="3.8"/>
        <testcase name="test_skipped_example" classname="$suite" time="0">
            <skipped message="Test not applicable in current environment"/>
        </testcase>
    </testsuite>
EOF
    done

    cat >> "$xml_report" << EOF
</testsuites>
EOF

    echo "  ‚úì JUnit XML: $(basename "$xml_report")"
}

# Generate Markdown summary
generate_markdown_summary() {
    local md_report="${REPORTS_DIR}/test_summary_$(date +%Y%m%d_%H%M%S).md"

    cat > "$md_report" << EOF
# VLESS+Reality VPN Test Execution Summary

**Generated:** $(date)
**Version:** 1.0.0
**Duration:** ${TEST_STATS["total_duration"]} seconds

## üìä Overall Results

| Metric | Value | Status |
|--------|-------|--------|
| Test Suites | ${TEST_STATS["total_suites"]} | $([ ${TEST_STATS["failed_suites"]} -eq 0 ] && echo "‚úÖ All Passed" || echo "‚ùå ${TEST_STATS["failed_suites"]} Failed") |
| Individual Tests | ${TEST_STATS["total_tests"]} | $([ ${TEST_STATS["failed_tests"]} -eq 0 ] && echo "‚úÖ All Passed" || echo "‚ùå ${TEST_STATS["failed_tests"]} Failed") |
| Success Rate | $(( TEST_STATS["total_tests"] > 0 ? TEST_STATS["passed_tests"] * 100 / TEST_STATS["total_tests"] : 0 ))% | $([ $((TEST_STATS["passed_tests"] * 100 / TEST_STATS["total_tests"])) -ge 95 ] && echo "üéØ Excellent" || echo "‚ö†Ô∏è Needs Improvement") |

## üìã Test Suite Breakdown

$([ ${TEST_STATS["passed_suites"]} -gt 0 ] && echo "### ‚úÖ Passed Suites (${TEST_STATS["passed_suites"]})")
$(for suite in "${!TEST_SUITES[@]}"; do echo "- $suite"; done)

$([ ${TEST_STATS["failed_suites"]} -gt 0 ] && echo "### ‚ùå Failed Suites (${TEST_STATS["failed_suites"]})")

$([ ${TEST_STATS["skipped_suites"]} -gt 0 ] && echo "### ‚è≠Ô∏è Skipped Suites (${TEST_STATS["skipped_suites"]})")

## üîç Test Categories

- **Unit Tests:** Validate individual module functionality
- **Integration Tests:** Verify component interactions
- **Security Tests:** Validate security controls and compliance
- **Performance Tests:** Benchmark system performance and scalability

## üìÅ Artifacts

- **Master Log:** \`$(basename "$MASTER_LOG_FILE")\`
- **Individual Logs:** \`${RESULTS_DIR}/*_*.log\`
- **Reports:** \`${REPORTS_DIR}/\`

## üöÄ Next Steps

$([ ${TEST_STATS["failed_tests"]} -eq 0 ] && echo "All tests passed! The system is ready for deployment." || echo "Review failed tests and address issues before deployment.")

---
*Generated by VLESS Master Test Runner v1.0.0*
EOF

    echo "  ‚úì Markdown summary: $(basename "$md_report")"
}

# Display final summary
display_final_summary() {
    local execution_end_time=$(date +%s)
    local total_duration=$((execution_end_time - EXECUTION_START_TIME))

    echo ""
    echo -e "${CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
    echo -e "${CYAN}‚ïë                     EXECUTION SUMMARY                         ‚ïë${NC}"
    echo -e "${CYAN}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
    echo ""

    # Test suite summary
    echo -e "${WHITE}Test Suite Results:${NC}"
    echo -e "  Total Suites:  ${BLUE}${TEST_STATS["total_suites"]}${NC}"
    echo -e "  Passed:        ${GREEN}${TEST_STATS["passed_suites"]}${NC}"
    echo -e "  Failed:        ${RED}${TEST_STATS["failed_suites"]}${NC}"
    echo -e "  Skipped:       ${YELLOW}${TEST_STATS["skipped_suites"]}${NC}"

    if [[ ${TEST_STATS["total_suites"]} -gt 0 ]]; then
        local suite_success_rate=$((TEST_STATS["passed_suites"] * 100 / TEST_STATS["total_suites"]))
        echo -e "  Success Rate:  ${BLUE}${suite_success_rate}%${NC}"
    fi

    echo ""

    # Individual test summary
    echo -e "${WHITE}Individual Test Results:${NC}"
    echo -e "  Total Tests:   ${BLUE}${TEST_STATS["total_tests"]}${NC}"
    echo -e "  Passed:        ${GREEN}${TEST_STATS["passed_tests"]}${NC}"
    echo -e "  Failed:        ${RED}${TEST_STATS["failed_tests"]}${NC}"
    echo -e "  Skipped:       ${YELLOW}${TEST_STATS["skipped_tests"]}${NC}"

    if [[ ${TEST_STATS["total_tests"]} -gt 0 ]]; then
        local test_success_rate=$((TEST_STATS["passed_tests"] * 100 / TEST_STATS["total_tests"]))
        echo -e "  Success Rate:  ${BLUE}${test_success_rate}%${NC}"
    fi

    echo ""

    # Timing information
    echo -e "${WHITE}Execution Time:${NC}"
    echo -e "  Total Duration: ${BLUE}${total_duration}s${NC}"
    echo -e "  Average per Suite: ${BLUE}$(( TEST_STATS["total_suites"] > 0 ? total_duration / TEST_STATS["total_suites"] : 0 ))s${NC}"

    echo ""

    # Final result
    if [[ ${TEST_STATS["failed_suites"]} -eq 0 && ${TEST_STATS["failed_tests"]} -eq 0 ]]; then
        echo -e "${GREEN}üéâ ALL TESTS PASSED! üéâ${NC}"
        echo -e "${GREEN}The system is ready for deployment.${NC}"
    elif [[ ${TEST_STATS["failed_suites"]} -eq 0 && ${TEST_STATS["failed_tests"]} -lt 5 ]]; then
        echo -e "${YELLOW}‚ö†Ô∏è  MOSTLY SUCCESSFUL ‚ö†Ô∏è${NC}"
        echo -e "${YELLOW}Minor issues detected. Review failed tests.${NC}"
    else
        echo -e "${RED}‚ùå TESTS FAILED ‚ùå${NC}"
        echo -e "${RED}Significant issues detected. Review and fix before deployment.${NC}"
    fi

    echo ""
    echo -e "${BLUE}üìÅ Results available in: ${WHITE}$RESULTS_DIR${NC}"
    if [[ "$GENERATE_REPORTS" == "true" ]]; then
        echo -e "${BLUE}üìä Reports available in: ${WHITE}$REPORTS_DIR${NC}"
    fi
    echo ""

    # Update master log with final summary
    {
        echo "EXECUTION SUMMARY"
        echo "================="
        echo "End Time: $(date)"
        echo "Total Duration: ${total_duration}s"
        echo "Suite Results: ${TEST_STATS["passed_suites"]} passed, ${TEST_STATS["failed_suites"]} failed, ${TEST_STATS["skipped_suites"]} skipped"
        echo "Test Results: ${TEST_STATS["passed_tests"]} passed, ${TEST_STATS["failed_tests"]} failed, ${TEST_STATS["skipped_tests"]} skipped"
        echo "Overall Status: $([ ${TEST_STATS["failed_suites"]} -eq 0 ] && echo "SUCCESS" || echo "FAILURE")"
        echo ""
    } >> "$MASTER_LOG_FILE"
}

# Main execution function
main() {
    local suites_to_run=()
    local run_type="all"

    # Parse command line arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_usage
                exit 0
                ;;
            -v|--verbose)
                VERBOSE_OUTPUT=true
                shift
                ;;
            -p|--parallel)
                PARALLEL_EXECUTION=true
                shift
                ;;
            -t|--timeout)
                TEST_TIMEOUT="$2"
                shift 2
                ;;
            -r|--no-reports)
                GENERATE_REPORTS=false
                shift
                ;;
            -l|--list)
                list_test_suites
                exit 0
                ;;
            -c|--clean)
                clean_test_results
                exit 0
                ;;
            -q|--quick)
                run_type="unit"
                shift
                ;;
            -f|--full)
                run_type="all"
                shift
                ;;
            -s|--suite)
                suites_to_run+=("$2")
                shift 2
                ;;
            unit|integration|security|performance|all)
                run_type="$1"
                shift
                ;;
            *)
                # Assume it's a specific test suite name
                suites_to_run+=("$1")
                shift
                ;;
        esac
    done

    # Initialize test runner
    initialize_test_runner

    # Determine which suites to run
    if [[ ${#suites_to_run[@]} -eq 0 ]]; then
        case "$run_type" in
            "unit")
                for suite in "${!TEST_SUITES[@]}"; do
                    [[ $suite == unit_* ]] && suites_to_run+=("$suite")
                done
                ;;
            "integration")
                for suite in "${!TEST_SUITES[@]}"; do
                    [[ $suite == integration_* ]] && suites_to_run+=("$suite")
                done
                ;;
            "security")
                for suite in "${!TEST_SUITES[@]}"; do
                    [[ $suite == security_* ]] && suites_to_run+=("$suite")
                done
                ;;
            "performance")
                for suite in "${!TEST_SUITES[@]}"; do
                    [[ $suite == performance_* ]] && suites_to_run+=("$suite")
                done
                ;;
            "all"|*)
                suites_to_run=($(printf "%s\n" "${!TEST_SUITES[@]}" | sort))
                ;;
        esac
    fi

    # Validate suites to run
    if [[ ${#suites_to_run[@]} -eq 0 ]]; then
        echo -e "${RED}‚ùå No test suites to run${NC}"
        exit 1
    fi

    echo -e "${CYAN}üìã Test suites to execute: ${#suites_to_run[@]}${NC}"
    for suite in "${suites_to_run[@]}"; do
        echo -e "  ${BLUE}‚óè${NC} $suite"
    done
    echo ""

    # Execute test suites
    if [[ "$PARALLEL_EXECUTION" == "true" && ${#suites_to_run[@]} -gt 1 ]]; then
        execute_suites_parallel "${suites_to_run[@]}"
    else
        execute_suites_sequential "${suites_to_run[@]}"
    fi

    # Generate reports
    generate_test_reports

    # Display final summary
    display_final_summary

    # Exit with appropriate code
    if [[ ${TEST_STATS["failed_suites"]} -eq 0 ]]; then
        exit 0
    else
        exit 1
    fi
}

# Only run main if script is executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi